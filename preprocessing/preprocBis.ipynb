{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specific-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms \n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "silver-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = '/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K/data.xlsx'\n",
    "#IMG_DIR = '/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K/Training Images/'\n",
    "\n",
    "DATA_PATH = '../archive/ODIR-5K/ODIR-5K/data.xlsx'\n",
    "#IMG_DIR = 'ODIR-5K/ODIR-5K/TRAINING/'\n",
    "IMG_DIR = '../archive/preprocessed_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "sixth-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(DATA_PATH)\n",
    "\n",
    "diagnostic_keyphrases = {'N': ['normal fundus'], \n",
    " 'D': ['nonproliferative retinopathy',\n",
    "  'non proliferative retinopathy','mild nonproliferative retinopathy',\n",
    "  'proliferative retinopathy'],\n",
    " 'G': ['glaucoma'],\n",
    " 'C': ['cataract'],\n",
    " 'A': ['age-related macular degeneration'],\n",
    " 'H': ['hypertensive'],\n",
    " 'M': ['myopi'],\n",
    " 'O': ['macular epiretinal membrane',\n",
    "  'epiretinal membrane',\n",
    "  'drusen',\n",
    "  'myelinated nerve fibers',\n",
    "  'vitreous degeneration',\n",
    "  'refractive media opacity',\n",
    "  'spotted membranous change',\n",
    "  'tessellated fundus',\n",
    "  'maculopathy',\n",
    "  'chorioretinal atrophy',\n",
    "  'branch retinal vein occlusion',\n",
    "  'retinal pigmentation',\n",
    "  'white vessel',\n",
    "  'post retinal laser surgery',\n",
    "  'epiretinal membrane over the macula',\n",
    "  'retinitis pigmentosa',\n",
    "  'central retinal vein occlusion',\n",
    "  'optic disc edema',\n",
    "  'post laser photocoagulation',\n",
    "  'retinochoroidal coloboma',\n",
    "  'atrophic change',\n",
    "  'optic nerve atrophy',\n",
    "  'old branch retinal vein occlusion',\n",
    "  'depigmentation of the retinal pigment epithelium',\n",
    "  'chorioretinal atrophy with pigmentation proliferation',\n",
    "  'central retinal artery occlusion',\n",
    "  'old chorioretinopathy',\n",
    "  'pigment epithelium proliferation',\n",
    "  'retina fold',\n",
    "  'abnormal pigment ',\n",
    "  'idiopathic choroidal neovascularization',\n",
    "  'branch retinal artery occlusion',\n",
    "  'vessel tortuosity',\n",
    "  'pigmentation disorder',\n",
    "  'rhegmatogenous retinal detachment',\n",
    "  'macular hole',\n",
    "  'morning glory syndrome',\n",
    "  'atrophy',\n",
    "  'laser spot',\n",
    "  'arteriosclerosis',\n",
    "  'asteroid hyalosis',\n",
    "  'congenital choroidal coloboma',\n",
    "  'macular coloboma',\n",
    "  'optic discitis',\n",
    "  'oval yellow-white atrophy',\n",
    "  'wedge-shaped change',\n",
    "  'wedge white line change',\n",
    "  'retinal artery macroaneurysm',\n",
    "  'retinal vascular sheathing',\n",
    "  'suspected abnormal color of  optic disc',\n",
    "  'suspected retinal vascular sheathing',\n",
    "  'suspected retinitis pigmentosa',\n",
    "  'silicone oil eye']}\n",
    "\n",
    "LeftText=[]\n",
    "for i, row in data.iterrows():\n",
    "    text=row['Left-Diagnostic Keywords']\n",
    "    Listecle=[]\n",
    "    for cle, valeur in diagnostic_keyphrases.items():\n",
    "        valeur = diagnostic_keyphrases.get (cle)\n",
    "        for keyword in valeur:\n",
    "            if keyword in text:\n",
    "                Listecle.append (cle)\n",
    "           \n",
    "    Listecle=list(set(Listecle))\n",
    "    LeftText.append (Listecle)\n",
    "\n",
    "data['Left Text']= LeftText\n",
    "\n",
    "\n",
    "RightText=[]\n",
    "for i, row in data.iterrows():\n",
    "    text=row['Right-Diagnostic Keywords']\n",
    "    Listecle=[]\n",
    "    for cle, valeur in diagnostic_keyphrases.items():\n",
    "        valeur = diagnostic_keyphrases.get (cle)\n",
    "        for keyword in valeur:\n",
    "            if keyword in text:\n",
    "                Listecle.append (cle)\n",
    "           \n",
    "    Listecle=list(set(Listecle))\n",
    "    RightText.append (Listecle)\n",
    "\n",
    "data['Right Text']= RightText\n",
    "key_columns = ['ID', 'Patient Age','Patient Sex','Left-Fundus','Right-Fundus','N','D','G','C','A','H','M','O','Left Text','Right Text'] \n",
    "data=data [key_columns]\n",
    "data.head()\n",
    "\n",
    "df=data\n",
    "df.to_csv('full_df.csv', mode='a', header=key_columns, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "weighted-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-korean",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "opposite-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementation du pseudo-code de Ben Graham\n",
    "\n",
    "def scaleRadius (img, scale):\n",
    "    toto=int(img.shape[0]/2)\n",
    "    x=img[toto,:,:].sum (1)\n",
    "    r=(x>x.mean()/10).sum()/2\n",
    "    s=scale * 1.0/r\n",
    "    return cv2.resize (img,(0,0), fx=s , fy=s )\n",
    "\n",
    "def preprocess_image(file_name):\n",
    "    scale =310\n",
    "    # lecture de l'image\n",
    "    image = cv2.imread(os.path.join(IMG_DIR, file_name))\n",
    " \n",
    "    #redimensionnement de l'image à un rayon donné\n",
    "    image=scaleRadius (image, scale)\n",
    "    \n",
    "    #on soustrait la couleur moyenne pour la mapper sur 50% de gris de façon à mieux faire ressortir les constrastes\n",
    "    image=cv2.addWeighted (image ,4,cv2.GaussianBlur (image,(0,0),scale/30),-4 ,128)\n",
    "    \n",
    "    #on enleve 10% des bordures\n",
    "    b=np.zeros(image.shape)\n",
    "\n",
    "    cv2.circle(b,(int(image.shape[1]/2), int(image.shape[0]/2)),int(scale * 0.9),(1,1,1),-1,8,0)\n",
    "    image=image*b+128*(1-b)\n",
    "    \n",
    "    cv2.imwrite(str(scale)+\"_\"+file_name ,image)\n",
    "\n",
    "   \n",
    "    return image\n",
    "\n",
    "def preprocess_patient(patient_id):\n",
    "    left_eye_file = str(patient_id) + '_left.jpg'\n",
    "    right_eye_file = str(patient_id) + '_right.jpg'\n",
    "    # combine image vertical/horizontal\n",
    "    image = cv2.hconcat([preprocess_image(left_eye_file), preprocess_image(right_eye_file)]) \n",
    "\n",
    "    return image\n",
    "\n",
    "#main_df = pd.read_csv('full_df.csv')\n",
    "#patient_id = main_df.iloc[13]['ID']\n",
    "#image = preprocess_patient(patient_id)\n",
    "image = preprocess_patient(13)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "spoken-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deuxieme implementation pour reduction size, normalisation, et enlever les bordures\n",
    "\n",
    "def crop(image): \n",
    "    # Remove vertical black borders (the image must be already normalized)\n",
    "    sums = image.sum(axis=0)\n",
    "    sums = sums.sum(axis=1)\n",
    "    filter_arr = []\n",
    "    for s in sums:\n",
    "        if s == 0:\n",
    "            filter_arr.append(False)\n",
    "        else:\n",
    "            filter_arr.append(True)\n",
    "    image = image[:, filter_arr]\n",
    "    \n",
    "    # Crop to a square shape\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]    \n",
    "    \n",
    "    if h < w:\n",
    "        x = (w - h)//2\n",
    "        image = image[:, x:x+h, :]        \n",
    "    elif h > w:\n",
    "        x = (h - w)//2\n",
    "        image = image[x:x+w, :, :]           \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def Xpreprocess_image(file_name):\n",
    "\n",
    "    # lecture de l'image\n",
    "    image = cv2.imread(os.path.join(IMG_DIR, file_name))\n",
    " \n",
    "\n",
    "    norm_img = np.zeros(image.shape)\n",
    "    # normalisation 0 ou 1\n",
    "    norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # enleve les bordures\n",
    "    image = crop(norm_img)\n",
    "\n",
    "    # redimension de l'image pour avoir meme dimension entre toutes les images (à cause image resolution differentes)\n",
    "    # et conversion de la couleur de l'image car par defaut cv2 lit image en couleur bleue\n",
    "    # dans le resize, le ration = 1 par defaut car hauteur pixel = largeur pixel\n",
    "    image = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    ##norm_img = np.zeros(image.shape)\n",
    "    # normalisation 0 ou 1\n",
    "    ##norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # enleve les bordures\n",
    "    ##image = crop(norm_img)\n",
    "      \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(str(\"X_\")+\"_\"+file_name ,image)\n",
    "      \n",
    "    return image\n",
    "\n",
    "def Xpreprocess_patient(patient_id):\n",
    "    left_eye_file = str(patient_id) + '_left.jpg'\n",
    "    right_eye_file = str(patient_id) + '_right.jpg'\n",
    "    # combine image vertical/horizontal\n",
    "    image = cv2.hconcat([Xpreprocess_image(left_eye_file), Xpreprocess_image(right_eye_file)]) \n",
    "\n",
    "    return image\n",
    "\n",
    "#patient_id = main_df.iloc[13]['ID']\n",
    "#image = Xpreprocess_patient(patient_id)    \n",
    "\n",
    "image = Xpreprocess_patient(13)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "local-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERIC --\n",
    "\n",
    "def preprocess_image_crop(image): \n",
    "    # Remove vertical black borders (the image must be already normalized)\n",
    "    sums = image.sum(axis=0)\n",
    "    sums = sums.sum(axis=1)\n",
    "    filter_arr = []\n",
    "    for s in sums:\n",
    "        if s == 0:\n",
    "            filter_arr.append(False)\n",
    "        else:\n",
    "            filter_arr.append(True)\n",
    "    image = image[:, filter_arr]\n",
    "    \n",
    "    # Crop to a square shape\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]    \n",
    "    \n",
    "    if h < w:\n",
    "        x = (w - h)//2\n",
    "        image = image[:, x:x+h, :]        \n",
    "    elif h > w:\n",
    "        x = (h - w)//2\n",
    "        image = image[x:x+w, :, :]           \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return image\n",
    "\n",
    "def preprocess_image_Resize(image):\n",
    "\n",
    "    norm_img = np.zeros(image.shape)\n",
    "    # normalisation 0 ou 1\n",
    "    norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # enleve les bordures\n",
    "    image = crop(norm_img)\n",
    "\n",
    "    # redimension de l'image pour avoir meme dimension entre toutes les images (à cause image resolution differentes)\n",
    "    # et conversion de la couleur de l'image car par defaut cv2 lit image en couleur bleue\n",
    "    # dans le resize, le ration = 1 par defaut car hauteur pixel = largeur pixel\n",
    "    image = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    ##norm_img = np.zeros(image.shape)\n",
    "    ##norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "      \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(str(\"X_\")+\"_\"+file_name ,image)\n",
    "      \n",
    "    return image\n",
    "\n",
    "def scaleRadius (img, scale):\n",
    "    toto=int(img.shape[0]/2)\n",
    "    x=img[toto,:,:].sum (1)\n",
    "    r=(x>x.mean()/10).sum()/2\n",
    "    s=scale * 1.0/r\n",
    "    return cv2.resize (img,(0,0), fx=s , fy=s )\n",
    "\n",
    "def preprocess_image_Ben (image):\n",
    "    \n",
    "    scale =320\n",
    "\n",
    "    norm_img = np.zeros(image.shape)\n",
    "    # normalisation 0 ou 1\n",
    "    norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # enleve les bordures\n",
    "    # #image = preprocess_image_crop(norm_img)\n",
    "\n",
    "    #redimensionnement de l'image à un rayon donné\n",
    "    image=scaleRadius (image, scale)\n",
    "    \n",
    "    #on soustrait la couleur moyenne pour la mapper sur 50% de gris de façon à mieux faire ressortir les constrastes\n",
    "    image=cv2.addWeighted (image ,4,cv2.GaussianBlur (image,(0,0),scale/30),-4 ,128)\n",
    "   \n",
    "   \n",
    "    #on enleve 10% des bordures\n",
    "    b=np.zeros(image.shape)\n",
    "\n",
    "    cv2.circle(b,(int(image.shape[1]/2), int(image.shape[0]/2)),int(scale * 0.95),(1,1,1),-1,8,0)\n",
    "    image=image*b+128*(1-b)\n",
    "    \n",
    "    \n",
    "    return image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "excess-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(color):\n",
    "    print (\"typezzzzz\", type(color))\n",
    "    #color = cv2.imread(os.path.join(IMG_DIR, filename))\n",
    "    #normalisation de l'image (valeurs entre 0 et 1)\n",
    "    normalized = np.zeros(color.shape)\n",
    "    normalized = cv2.normalize(color,  normalized, 0, 255, cv2.NORM_MINMAX)\n",
    "    #suppression des bordures\n",
    "    croped = crop(normalized)\n",
    "    # passage en noir et blanc\n",
    "    gray = cv2.cvtColor(croped, cv2.COLOR_BGR2GRAY)\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "union-sperm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typezzzzz <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = str(13) + '_left.jpg'\n",
    "# lecture de l'image\n",
    "image = cv2.imread(os.path.join(IMG_DIR, file_name))\n",
    "#image=preprocess_image_Ben(image)\n",
    "#image=preprocess_image_crop (image)\n",
    "image=to_grayscale(image)\n",
    "cv2.imwrite(str(800)+\"_\"+file_name ,image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "italic-bachelor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_images/\n",
      "['1_right.jpg' '2_right.jpg' '3_right.jpg' '4_right.jpg' '5_right.jpg'\n",
      " '6_right.jpg' '7_right.jpg' '8_right.jpg' '9_right.jpg']\n",
      "preprocessed_images/0_right.jpg\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'JpegImageFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-45f21c7c682e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mimg_num\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-45f21c7c682e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mimg_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mlabels_unique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-77a13e9b8ff3>\u001b[0m in \u001b[0;36mto_grayscale\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#color = cv2.imread(os.path.join(IMG_DIR, filename))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#normalisation de l'image (valeurs entre 0 et 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnormalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNORM_MINMAX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#suppression des bordures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'JpegImageFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data_dir,transform):\n",
    "        'Initialization'\n",
    "\n",
    "        \"\"\"\n",
    "        to_drop = ['ID', 'Patient Age', 'Patient Sex', 'Left-Fundus', 'Right-Fundus',\n",
    "           'Left-Diagnostic Keywords', 'Right-Diagnostic Keywords', 'N', 'D', 'G',\n",
    "           'C', 'A', 'H', 'M', 'O', 'filepath', 'target']\n",
    "        data = data.drop(columns = to_drop)\n",
    "        \"\"\"\n",
    "\n",
    "        data = pd.read_csv('full_df.csv')\n",
    "\n",
    "        my_dir = data_dir+'preprocessed_images/'\n",
    "        print(my_dir)\n",
    "        my_list = glob.glob(os.path.join(my_dir,'*.jpg'))\n",
    "        \n",
    "        right_name_list = data['Right-Fundus'].to_numpy()\n",
    "        left_name_list = data['Left-Fundus'].to_numpy()\n",
    "        filename_list = np.concatenate((right_name_list, left_name_list), axis=None)\n",
    "        right_labels_list = data['Right Text'].to_numpy()\n",
    "        left_labels_list = data['Left Text'].to_numpy()\n",
    "        labels_list = np.concatenate((right_labels_list, left_labels_list), axis=None)\n",
    "\n",
    "        self.labels = labels_list\n",
    "        self.list_IDs = filename_list\n",
    "        self.transform = transform\n",
    "        print(self.list_IDs[1:10])\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        'Generates one sample of data'\n",
    "        print(\"ICI\")\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "        # Load data and get label\n",
    "        X = torch.load('data/preprocessed_images/' + ID)\n",
    "        y = self.labels[ID]\n",
    "        return X, y\n",
    "        \"\"\"\n",
    "        ID = self.list_IDs[index]\n",
    "        img_path = 'preprocessed_images/' + ID\n",
    "        img = Image.open(img_path)\n",
    "        print (img_path)\n",
    "        img_transformed = self.transform(img)\n",
    "\n",
    "        labels_unique = np.unique(self.labels)\n",
    "        label = self.labels[index]\n",
    "        label = np.where(labels_unique == label)[0][0]\n",
    "        #print(self.label_list[idx], label)\n",
    "\n",
    "        return img_transformed, label\n",
    "\n",
    "\n",
    "#my_transforms = transforms.Compose ([\n",
    "#    transforms.RandomRotation (degrees=45),\n",
    "#    transforms.RandomHorizontalFlip(p=0.5),\n",
    "#    transforms.ToTensor(),\n",
    "#    transforms.Normalize (mean=[0.485,0.456,0.406],\n",
    "#                          std=[0.229,0.224,0.225])\n",
    "#])\n",
    "\n",
    "my_transforms = transforms.Compose ([\n",
    "    to_grayscale,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize (mean=[0.485,0.456,0.406],\n",
    "                          std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset=Dataset(data_dir='', transform=my_transforms)\n",
    "\n",
    "img_num=0\n",
    "for img, label in dataset:\n",
    "    img_num +=1\n",
    "img_num=img_num-1\n",
    "save_image (img, tutu.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "arctic-equilibrium",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'full_df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-39c87c43f7bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m ])\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#trainset, testset = torch.utils.data.random_split(dataset, 0.8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-c6457e634e09>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, transform)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \"\"\"\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'full_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmy_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'preprocessed_images/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'full_df.csv'"
     ]
    }
   ],
   "source": [
    "my_transforms = transforms.Compose ([\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize (mean=[0.485,0.456,0.406],\n",
    "                          std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset=Dataset(data_dir='', transform=my_transforms)\n",
    "#trainset, testset = torch.utils.data.random_split(dataset, 0.8)\n",
    "\n",
    "#for i, row in data.iterrows():\n",
    "#    LeftImage=row['Left-Fundus']\n",
    "#    RightImage=row['Right-Fundus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-sender",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
