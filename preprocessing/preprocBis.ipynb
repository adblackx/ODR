{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "specific-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms \n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "silver-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = '/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K/data.xlsx'\n",
    "#IMG_DIR = '/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K/Training Images/'\n",
    "\n",
    "DATA_PATH = '../archive/ODIR-5K/ODIR-5K/data.xlsx'\n",
    "#IMG_DIR = 'ODIR-5K/ODIR-5K/TRAINING/'\n",
    "IMG_DIR = '../archive/preprocessed_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sixth-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(DATA_PATH)\n",
    "\n",
    "diagnostic_keyphrases = {'N': ['normal fundus'], \n",
    " 'D': ['nonproliferative retinopathy',\n",
    "  'non proliferative retinopathy','mild nonproliferative retinopathy',\n",
    "  'proliferative retinopathy'],\n",
    " 'G': ['glaucoma'],\n",
    " 'C': ['cataract'],\n",
    " 'A': ['age-related macular degeneration'],\n",
    " 'H': ['hypertensive'],\n",
    " 'M': ['myopi'],\n",
    " 'O': ['macular epiretinal membrane',\n",
    "  'epiretinal membrane',\n",
    "  'drusen',\n",
    "  'myelinated nerve fibers',\n",
    "  'vitreous degeneration',\n",
    "  'refractive media opacity',\n",
    "  'spotted membranous change',\n",
    "  'tessellated fundus',\n",
    "  'maculopathy',\n",
    "  'chorioretinal atrophy',\n",
    "  'branch retinal vein occlusion',\n",
    "  'retinal pigmentation',\n",
    "  'white vessel',\n",
    "  'post retinal laser surgery',\n",
    "  'epiretinal membrane over the macula',\n",
    "  'retinitis pigmentosa',\n",
    "  'central retinal vein occlusion',\n",
    "  'optic disc edema',\n",
    "  'post laser photocoagulation',\n",
    "  'retinochoroidal coloboma',\n",
    "  'atrophic change',\n",
    "  'optic nerve atrophy',\n",
    "  'old branch retinal vein occlusion',\n",
    "  'depigmentation of the retinal pigment epithelium',\n",
    "  'chorioretinal atrophy with pigmentation proliferation',\n",
    "  'central retinal artery occlusion',\n",
    "  'old chorioretinopathy',\n",
    "  'pigment epithelium proliferation',\n",
    "  'retina fold',\n",
    "  'abnormal pigment ',\n",
    "  'idiopathic choroidal neovascularization',\n",
    "  'branch retinal artery occlusion',\n",
    "  'vessel tortuosity',\n",
    "  'pigmentation disorder',\n",
    "  'rhegmatogenous retinal detachment',\n",
    "  'macular hole',\n",
    "  'morning glory syndrome',\n",
    "  'atrophy',\n",
    "  'laser spot',\n",
    "  'arteriosclerosis',\n",
    "  'asteroid hyalosis',\n",
    "  'congenital choroidal coloboma',\n",
    "  'macular coloboma',\n",
    "  'optic discitis',\n",
    "  'oval yellow-white atrophy',\n",
    "  'wedge-shaped change',\n",
    "  'wedge white line change',\n",
    "  'retinal artery macroaneurysm',\n",
    "  'retinal vascular sheathing',\n",
    "  'suspected abnormal color of  optic disc',\n",
    "  'suspected retinal vascular sheathing',\n",
    "  'suspected retinitis pigmentosa',\n",
    "  'silicone oil eye']}\n",
    "\n",
    "LeftText=[]\n",
    "for i, row in data.iterrows():\n",
    "    text=row['Left-Diagnostic Keywords']\n",
    "    Listecle=[]\n",
    "    for cle, valeur in diagnostic_keyphrases.items():\n",
    "        valeur = diagnostic_keyphrases.get (cle)\n",
    "        for keyword in valeur:\n",
    "            if keyword in text:\n",
    "                Listecle.append (cle)\n",
    "           \n",
    "    Listecle=list(set(Listecle))\n",
    "    LeftText.append (Listecle)\n",
    "\n",
    "data['Left Text']= LeftText\n",
    "\n",
    "\n",
    "RightText=[]\n",
    "for i, row in data.iterrows():\n",
    "    text=row['Right-Diagnostic Keywords']\n",
    "    Listecle=[]\n",
    "    for cle, valeur in diagnostic_keyphrases.items():\n",
    "        valeur = diagnostic_keyphrases.get (cle)\n",
    "        for keyword in valeur:\n",
    "            if keyword in text:\n",
    "                Listecle.append (cle)\n",
    "           \n",
    "    Listecle=list(set(Listecle))\n",
    "    RightText.append (Listecle)\n",
    "\n",
    "data['Right Text']= RightText\n",
    "key_columns = ['ID', 'Patient Age','Patient Sex','Left-Fundus','Right-Fundus','N','D','G','C','A','H','M','O','Left Text','Right Text'] \n",
    "data=data [key_columns]\n",
    "data.head()\n",
    "\n",
    "df=data\n",
    "df.to_csv('full_df.csv', mode='a', header=key_columns, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "weighted-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-korean",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opposite-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementation du pseudo-code de Ben Graham\n",
    "\n",
    "def scaleRadius (img, scale):\n",
    "    toto=int(img.shape[0]/2)\n",
    "    x=img[toto,:,:].sum (1)\n",
    "    r=(x>x.mean()/10).sum()/2\n",
    "    s=scale * 1.0/r\n",
    "    return cv2.resize (img,(0,0), fx=s , fy=s )\n",
    "\n",
    "def preprocess_image(file_name):\n",
    "    scale =310\n",
    "    # lecture de l'image\n",
    "    image = cv2.imread(os.path.join(IMG_DIR, file_name))\n",
    " \n",
    "    #redimensionnement de l'image à un rayon donné\n",
    "    image=scaleRadius (image, scale)\n",
    "    \n",
    "    #on soustrait la couleur moyenne pour la mapper sur 50% de gris de façon à mieux faire ressortir les constrastes\n",
    "    image=cv2.addWeighted (image ,4,cv2.GaussianBlur (image,(0,0),scale/30),-4 ,128)\n",
    "    \n",
    "    #on enleve 10% des bordures\n",
    "    b=np.zeros(image.shape)\n",
    "\n",
    "    cv2.circle(b,(int(image.shape[1]/2), int(image.shape[0]/2)),int(scale * 0.9),(1,1,1),-1,8,0)\n",
    "    image=image*b+128*(1-b)\n",
    "    \n",
    "    cv2.imwrite(str(scale)+\"_\"+file_name ,image)\n",
    "\n",
    "   \n",
    "    return image\n",
    "\n",
    "def preprocess_patient(patient_id):\n",
    "    left_eye_file = str(patient_id) + '_left.jpg'\n",
    "    right_eye_file = str(patient_id) + '_right.jpg'\n",
    "    # combine image vertical/horizontal\n",
    "    image = cv2.hconcat([preprocess_image(left_eye_file), preprocess_image(right_eye_file)]) \n",
    "\n",
    "    return image\n",
    "\n",
    "#main_df = pd.read_csv('full_df.csv')\n",
    "#patient_id = main_df.iloc[13]['ID']\n",
    "#image = preprocess_patient(patient_id)\n",
    "image = preprocess_patient(13)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spoken-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deuxieme implementation pour reduction size, normalisation, et enlever les bordures\n",
    "\n",
    "def crop(image): \n",
    "    # Remove vertical black borders (the image must be already normalized)\n",
    "    sums = image.sum(axis=0)\n",
    "    sums = sums.sum(axis=1)\n",
    "    filter_arr = []\n",
    "    for s in sums:\n",
    "        if s == 0:\n",
    "            filter_arr.append(False)\n",
    "        else:\n",
    "            filter_arr.append(True)\n",
    "    image = image[:, filter_arr]\n",
    "    \n",
    "    # Crop to a square shape\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]    \n",
    "    \n",
    "    if h < w:\n",
    "        x = (w - h)//2\n",
    "        image = image[:, x:x+h, :]        \n",
    "    elif h > w:\n",
    "        x = (h - w)//2\n",
    "        image = image[x:x+w, :, :]           \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def Xpreprocess_image(file_name):\n",
    "\n",
    "    # lecture de l'image\n",
    "    image = cv2.imread(os.path.join(IMG_DIR, file_name))\n",
    " \n",
    "\n",
    "    norm_img = np.zeros(image.shape)\n",
    "    # normalisation 0 ou 1\n",
    "    norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # enleve les bordures\n",
    "    image = crop(norm_img)\n",
    "\n",
    "    # redimension de l'image pour avoir meme dimension entre toutes les images (à cause image resolution differentes)\n",
    "    # et conversion de la couleur de l'image car par defaut cv2 lit image en couleur bleue\n",
    "    # dans le resize, le ration = 1 par defaut car hauteur pixel = largeur pixel\n",
    "    image = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    ##norm_img = np.zeros(image.shape)\n",
    "    # normalisation 0 ou 1\n",
    "    ##norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # enleve les bordures\n",
    "    ##image = crop(norm_img)\n",
    "      \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(str(\"X_\")+\"_\"+file_name ,image)\n",
    "      \n",
    "    return image\n",
    "\n",
    "def Xpreprocess_patient(patient_id):\n",
    "    left_eye_file = str(patient_id) + '_left.jpg'\n",
    "    right_eye_file = str(patient_id) + '_right.jpg'\n",
    "    # combine image vertical/horizontal\n",
    "    image = cv2.hconcat([Xpreprocess_image(left_eye_file), Xpreprocess_image(right_eye_file)]) \n",
    "\n",
    "    return image\n",
    "\n",
    "#patient_id = main_df.iloc[13]['ID']\n",
    "#image = Xpreprocess_patient(patient_id)    \n",
    "\n",
    "image = Xpreprocess_patient(13)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "local-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERIC --\n",
    "\n",
    "def preprocess_image_crop(image): \n",
    "    # Remove vertical black borders (the image must be already normalized)\n",
    "    sums = image.sum(axis=0)\n",
    "    sums = sums.sum(axis=1)\n",
    "    filter_arr = []\n",
    "    for s in sums:\n",
    "        if s == 0:\n",
    "            filter_arr.append(False)\n",
    "        else:\n",
    "            filter_arr.append(True)\n",
    "    image = image[:, filter_arr]\n",
    "    \n",
    "    # Crop to a square shape\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]    \n",
    "    \n",
    "    if h < w:\n",
    "        x = (w - h)//2\n",
    "        image = image[:, x:x+h, :]        \n",
    "    elif h > w:\n",
    "        x = (h - w)//2\n",
    "        image = image[x:x+w, :, :]           \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return image\n",
    "\n",
    "def preprocess_image_Resize(image):\n",
    "\n",
    "    norm_img = np.zeros(image.shape)\n",
    "    # normalisation 0 ou 1\n",
    "    norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # enleve les bordures\n",
    "    image = crop(norm_img)\n",
    "\n",
    "    # redimension de l'image pour avoir meme dimension entre toutes les images (à cause image resolution differentes)\n",
    "    # et conversion de la couleur de l'image car par defaut cv2 lit image en couleur bleue\n",
    "    # dans le resize, le ration = 1 par defaut car hauteur pixel = largeur pixel\n",
    "    image = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    ##norm_img = np.zeros(image.shape)\n",
    "    ##norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "      \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(str(\"X_\")+\"_\"+file_name ,image)\n",
    "      \n",
    "    return image\n",
    "\n",
    "def scaleRadius (img, scale):\n",
    "    toto=int(img.shape[0]/2)\n",
    "    x=img[toto,:,:].sum (1)\n",
    "    r=(x>x.mean()/10).sum()/2\n",
    "    s=scale * 1.0/r\n",
    "    return cv2.resize (img,(0,0), fx=s , fy=s )\n",
    "\n",
    "def preprocess_image_Ben (image):\n",
    "    \n",
    "    scale =320\n",
    "\n",
    "    norm_img = np.zeros(image.shape)\n",
    "    # normalisation 0 ou 1\n",
    "    norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # enleve les bordures\n",
    "    # #image = preprocess_image_crop(norm_img)\n",
    "\n",
    "    #redimensionnement de l'image à un rayon donné\n",
    "    image=scaleRadius (image, scale)\n",
    "    \n",
    "    #on soustrait la couleur moyenne pour la mapper sur 50% de gris de façon à mieux faire ressortir les constrastes\n",
    "    image=cv2.addWeighted (image ,4,cv2.GaussianBlur (image,(0,0),scale/30),-4 ,128)\n",
    "   \n",
    "   \n",
    "    #on enleve 10% des bordures\n",
    "    b=np.zeros(image.shape)\n",
    "\n",
    "    cv2.circle(b,(int(image.shape[1]/2), int(image.shape[0]/2)),int(scale * 0.95),(1,1,1),-1,8,0)\n",
    "    image=image*b+128*(1-b)\n",
    "    \n",
    "    \n",
    "    return image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "excess-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(color):\n",
    "    #suppression des bordures\n",
    "    # passage en noir et blanc\n",
    "    return ImageOps.grayscale(croped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "union-sperm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typezzzzz <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = str(13) + '_left.jpg'\n",
    "# lecture de l'image\n",
    "image = cv2.imread(os.path.join(IMG_DIR, file_name))\n",
    "#image=preprocess_image_Ben(image)\n",
    "#image=preprocess_image_crop (image)\n",
    "image=to_grayscale(image)\n",
    "cv2.imwrite(str(800)+\"_\"+file_name ,image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "italic-bachelor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_images/\n",
      "['1_right.jpg' '2_right.jpg' '3_right.jpg' '4_right.jpg' '5_right.jpg'\n",
      " '6_right.jpg' '7_right.jpg' '8_right.jpg' '9_right.jpg']\n",
      "../archive/preprocessed_images/0_right.jpg\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'JpegImageFile' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-40f6341e693f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mimg_num\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-40f6341e693f>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mimg_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mlabels_unique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-08158eb2bf00>\u001b[0m in \u001b[0;36mto_grayscale\u001b[0;34m(color)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#suppression des bordures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcroped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# passage en noir et blanc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mImageOps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcroped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-dd911833aaa1>\u001b[0m in \u001b[0;36mcrop\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Remove vertical black borders (the image must be already normalized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfilter_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'JpegImageFile' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data_dir,transform):\n",
    "        'Initialization'\n",
    "\n",
    "        \"\"\"\n",
    "        to_drop = ['ID', 'Patient Age', 'Patient Sex', 'Left-Fundus', 'Right-Fundus',\n",
    "           'Left-Diagnostic Keywords', 'Right-Diagnostic Keywords', 'N', 'D', 'G',\n",
    "           'C', 'A', 'H', 'M', 'O', 'filepath', 'target']\n",
    "        data = data.drop(columns = to_drop)\n",
    "        \"\"\"\n",
    "\n",
    "        data = pd.read_csv('full_df.csv')\n",
    "\n",
    "        my_dir = data_dir+'preprocessed_images/'\n",
    "        print(my_dir)\n",
    "        my_list = glob.glob(os.path.join(my_dir,'*.jpg'))\n",
    "        \n",
    "        right_name_list = data['Right-Fundus'].to_numpy()\n",
    "        left_name_list = data['Left-Fundus'].to_numpy()\n",
    "        filename_list = np.concatenate((right_name_list, left_name_list), axis=None)\n",
    "        right_labels_list = data['Right Text'].to_numpy()\n",
    "        left_labels_list = data['Left Text'].to_numpy()\n",
    "        labels_list = np.concatenate((right_labels_list, left_labels_list), axis=None)\n",
    "\n",
    "        self.labels = labels_list\n",
    "        self.list_IDs = filename_list\n",
    "        self.transform = transform\n",
    "        print(self.list_IDs[1:10])\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        'Generates one sample of data'\n",
    "        print(\"ICI\")\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "        # Load data and get label\n",
    "        X = torch.load('data/preprocessed_images/' + ID)\n",
    "        y = self.labels[ID]\n",
    "        return X, y\n",
    "        \"\"\"\n",
    "        ID = self.list_IDs[index]\n",
    "        img_path = '../archive/preprocessed_images/' + ID\n",
    "        img = Image.open(img_path)\n",
    "        print (img_path)\n",
    "        img_transformed = self.transform(img)\n",
    "\n",
    "        labels_unique = np.unique(self.labels)\n",
    "        label = self.labels[index]\n",
    "        label = np.where(labels_unique == label)[0][0]\n",
    "        #print(self.label_list[idx], label)\n",
    "\n",
    "        return img_transformed, label\n",
    "\n",
    "\n",
    "#my_transforms = transforms.Compose ([\n",
    "#    transforms.RandomRotation (degrees=45),\n",
    "#    transforms.RandomHorizontalFlip(p=0.5),\n",
    "#    transforms.ToTensor(),\n",
    "#    transforms.Normalize (mean=[0.485,0.456,0.406],\n",
    "#                          std=[0.229,0.224,0.225])\n",
    "#])\n",
    "\n",
    "my_transforms = transforms.Compose ([\n",
    "    to_grayscale,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize (mean=0.5,\n",
    "                          std=0.229)\n",
    "])\n",
    "\n",
    "dataset=Dataset(data_dir='', transform=my_transforms)\n",
    "\n",
    "img_num=0\n",
    "for img, label in dataset:\n",
    "    img_num +=1\n",
    "img_num=img_num-1\n",
    "save_image (img, tutu.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "arctic-equilibrium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_images/\n",
      "['1_right.jpg' '2_right.jpg' '3_right.jpg' '4_right.jpg' '5_right.jpg'\n",
      " '6_right.jpg' '7_right.jpg' '8_right.jpg' '9_right.jpg']\n"
     ]
    }
   ],
   "source": [
    "my_transforms = transforms.Compose ([\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize (mean=[0.485,0.456,0.406],\n",
    "                          std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset=Dataset(data_dir='', transform=my_transforms)\n",
    "#trainset, testset = torch.utils.data.random_split(dataset, 0.8)\n",
    "\n",
    "#for i, row in data.iterrows():\n",
    "#    LeftImage=row['Left-Fundus']\n",
    "#    RightImage=row['Right-Fundus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-sender",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-acquisition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-civilization",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
