{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "virtual-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms \n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "severe-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = '/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K/data.xlsx'\n",
    "#IMG_DIR = '/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K/Training Images/'\n",
    "\n",
    "DATA_PATH = 'ODIR-5K/ODIR-5K/data.xlsx'\n",
    "#IMG_DIR = 'ODIR-5K/ODIR-5K/TRAINING/'\n",
    "IMG_DIR = 'preprocessed_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "valued-savage",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'xfull_df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-94b0181db912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#data = pd.read_excel(DATA_PATH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"xfull_df.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m diagnostic_keyphrases = {'N': ['normal fundus'], \n\u001b[1;32m      5\u001b[0m  'D': ['nonproliferative retinopathy',\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'xfull_df.csv'"
     ]
    }
   ],
   "source": [
    "#data = pd.read_excel(DATA_PATH)\n",
    "data=pd.read_csv (\"xfull_df.csv\")\n",
    "\n",
    "diagnostic_keyphrases = {'N': ['normal fundus'], \n",
    " 'D': ['nonproliferative retinopathy',\n",
    "  'non proliferative retinopathy','mild nonproliferative retinopathy',\n",
    "  'proliferative retinopathy'],\n",
    " 'G': ['glaucoma'],\n",
    " 'C': ['cataract'],\n",
    " 'A': ['age-related macular degeneration'],\n",
    " 'H': ['hypertensive'],\n",
    " 'M': ['myopi'],\n",
    " 'O': ['macular epiretinal membrane',\n",
    "  'epiretinal membrane',\n",
    "  'drusen',\n",
    "  'myelinated nerve fibers',\n",
    "  'vitreous degeneration',\n",
    "  'refractive media opacity',\n",
    "  'spotted membranous change',\n",
    "  'tessellated fundus',\n",
    "  'maculopathy',\n",
    "  'chorioretinal atrophy',\n",
    "  'branch retinal vein occlusion',\n",
    "  'retinal pigmentation',\n",
    "  'white vessel',\n",
    "  'post retinal laser surgery',\n",
    "  'epiretinal membrane over the macula',\n",
    "  'retinitis pigmentosa',\n",
    "  'central retinal vein occlusion',\n",
    "  'optic disc edema',\n",
    "  'post laser photocoagulation',\n",
    "  'retinochoroidal coloboma',\n",
    "  'atrophic change',\n",
    "  'optic nerve atrophy',\n",
    "  'old branch retinal vein occlusion',\n",
    "  'depigmentation of the retinal pigment epithelium',\n",
    "  'chorioretinal atrophy with pigmentation proliferation',\n",
    "  'central retinal artery occlusion',\n",
    "  'old chorioretinopathy',\n",
    "  'pigment epithelium proliferation',\n",
    "  'retina fold',\n",
    "  'abnormal pigment ',\n",
    "  'idiopathic choroidal neovascularization',\n",
    "  'branch retinal artery occlusion',\n",
    "  'vessel tortuosity',\n",
    "  'pigmentation disorder',\n",
    "  'rhegmatogenous retinal detachment',\n",
    "  'macular hole',\n",
    "  'morning glory syndrome',\n",
    "  'atrophy',\n",
    "  'laser spot',\n",
    "  'arteriosclerosis',\n",
    "  'asteroid hyalosis',\n",
    "  'congenital choroidal coloboma',\n",
    "  'macular coloboma',\n",
    "  'optic discitis',\n",
    "  'oval yellow-white atrophy',\n",
    "  'wedge-shaped change',\n",
    "  'wedge white line change',\n",
    "  'retinal artery macroaneurysm',\n",
    "  'retinal vascular sheathing',\n",
    "  'suspected abnormal color of  optic disc',\n",
    "  'suspected retinal vascular sheathing',\n",
    "  'suspected retinitis pigmentosa',\n",
    "  'silicone oil eye']}\n",
    "\n",
    "LeftText=[]\n",
    "for i, row in data.iterrows():\n",
    "    text=row['Left-Diagnostic Keywords']\n",
    "    Listecle=[]\n",
    "    for cle, valeur in diagnostic_keyphrases.items():\n",
    "        valeur = diagnostic_keyphrases.get (cle)\n",
    "        for keyword in valeur:\n",
    "            if keyword in text:\n",
    "                Listecle.append (cle)\n",
    "           \n",
    "    Listecle=list(set(Listecle))\n",
    "    LeftText.append (Listecle)\n",
    "\n",
    "data['Left Text']= LeftText\n",
    "\n",
    "\n",
    "RightText=[]\n",
    "for i, row in data.iterrows():\n",
    "    text=row['Right-Diagnostic Keywords']\n",
    "    Listecle=[]\n",
    "    for cle, valeur in diagnostic_keyphrases.items():\n",
    "        valeur = diagnostic_keyphrases.get (cle)\n",
    "        for keyword in valeur:\n",
    "            if keyword in text:\n",
    "                Listecle.append (cle)\n",
    "           \n",
    "    Listecle=list(set(Listecle))\n",
    "    RightText.append (Listecle)\n",
    "\n",
    "data['Right Text']= RightText\n",
    "key_columns = ['ID','Patient Age','Patient Sex','Left-Fundus','Right-Fundus','N','D','G','C','A','H','M','O','Left Text','Right Text'] \n",
    "data=data [key_columns]\n",
    "data.head()\n",
    "\n",
    "df=pd.DataFrame(data=data, columns=['ID','Patient Age','Patient Sex','Left-Fundus','Right-Fundus','N','D','G','C','A','H','M','O','Left Text','Right Text'])\n",
    "df.to_csv('toto.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "enormous-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thousand-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementation du pseudo-code de Ben Graham\n",
    "\n",
    "def scaleRadius (img, scale):\n",
    "    toto=int(img.shape[0]/2)\n",
    "    x=img[toto,:,:].sum (1)\n",
    "    r=(x>x.mean()/10).sum()/2\n",
    "    s=scale * 1.0/r\n",
    "    return cv2.resize (img,(0,0), fx=s , fy=s )\n",
    "\n",
    "def preprocess_image(file_name):\n",
    "    scale =310\n",
    "    # lecture de l'image\n",
    "    image = cv2.imread(os.path.join(IMG_DIR, file_name))\n",
    " \n",
    "    #redimensionnement de l'image à un rayon donné\n",
    "    image=scaleRadius (image, scale)\n",
    "    \n",
    "    #on soustrait la couleur moyenne pour la mapper sur 50% de gris de façon à mieux faire ressortir les constrastes\n",
    "    #image=cv2.addWeighted (image ,4,cv2.GaussianBlur (image,(0,0),scale/30),-4 ,128)\n",
    "    image=cv2.addWeighted (image ,4,cv2.GaussianBlur (image,(0,0),scale/30),-4 ,128)\n",
    "    \n",
    "    #on enleve 10% des bordures\n",
    "    b=np.zeros(image.shape)\n",
    "\n",
    "    cv2.circle(b,(int(image.shape[1]/2), int(image.shape[0]/2)),int(scale * 0.9),(1,1,1),-1,8,0)\n",
    "    image=image*b+128*(1-b)\n",
    "    \n",
    "    cv2.imwrite(str(scale)+\"_\"+file_name ,image)\n",
    "\n",
    "   \n",
    "    return image\n",
    "\n",
    "def preprocess_patient(patient_id):\n",
    "    left_eye_file = str(patient_id) + '_left.jpg'\n",
    "    right_eye_file = str(patient_id) + '_right.jpg'\n",
    "    # combine image vertical/horizontal\n",
    "    image = cv2.hconcat([preprocess_image(left_eye_file), preprocess_image(right_eye_file)]) \n",
    "\n",
    "    return image\n",
    "\n",
    "patient_id = main_df.iloc[13]['ID']\n",
    "image = preprocess_patient(patient_id)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "color-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deuxieme implementation pour reduction size, normalisation, et enlever les bordures\n",
    "\n",
    "def crop(image): \n",
    "    # Remove vertical black borders (the image must be already normalized)\n",
    "    sums = image.sum(axis=0)\n",
    "    sums = sums.sum(axis=1)\n",
    "    filter_arr = []\n",
    "    for s in sums:\n",
    "        if s == 0:\n",
    "            filter_arr.append(False)\n",
    "        else:\n",
    "            filter_arr.append(True)\n",
    "    image = image[:, filter_arr]\n",
    "    \n",
    "    # Crop to a square shape\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]    \n",
    "    \n",
    "    if h < w:\n",
    "        x = (w - h)//2\n",
    "        image = image[:, x:x+h, :]        \n",
    "    elif h > w:\n",
    "        x = (h - w)//2\n",
    "        image = image[x:x+w, :, :]           \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def Xpreprocess_image(file_name):\n",
    "\n",
    "    # lecture de l'image\n",
    "    image = cv2.imread(os.path.join(IMG_DIR, file_name))\n",
    " \n",
    "\n",
    "    norm_img = np.zeros(image.shape)\n",
    "    # normalisation 0 ou 1\n",
    "    norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # enleve les bordures\n",
    "    image = crop(norm_img)\n",
    "\n",
    "    # redimension de l'image pour avoir meme dimension entre toutes les images (à cause image resolution differentes)\n",
    "    # et conversion de la couleur de l'image car par defaut cv2 lit image en couleur bleue\n",
    "    # dans le resize, le ration = 1 par defaut car hauteur pixel = largeur pixel\n",
    "    image = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    ##norm_img = np.zeros(image.shape)\n",
    "    # normalisation 0 ou 1\n",
    "    ##norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # enleve les bordures\n",
    "    ##image = crop(norm_img)\n",
    "      \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(str(\"X_\")+\"_\"+file_name ,image)\n",
    "      \n",
    "    return image\n",
    "\n",
    "def Xpreprocess_patient(patient_id):\n",
    "    left_eye_file = str(patient_id) + '_left.jpg'\n",
    "    right_eye_file = str(patient_id) + '_right.jpg'\n",
    "    # combine image vertical/horizontal\n",
    "    image = cv2.hconcat([Xpreprocess_image(left_eye_file), Xpreprocess_image(right_eye_file)]) \n",
    "\n",
    "    return image\n",
    "\n",
    "patient_id = main_df.iloc[13]['ID']\n",
    "image = Xpreprocess_patient(patient_id)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "parliamentary-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Troisieme implementation \n",
    "\n",
    "def crop(image): \n",
    "    # Remove vertical black borders (the image must be already normalized)\n",
    "    sums = image.sum(axis=0)\n",
    "    sums = sums.sum(axis=1)\n",
    "    filter_arr = []\n",
    "    for s in sums:\n",
    "        if s == 0:\n",
    "            filter_arr.append(False)\n",
    "        else:\n",
    "            filter_arr.append(True)\n",
    "    image = image[:, filter_arr]\n",
    "    \n",
    "    # Crop to a square shape\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]    \n",
    "    \n",
    "    if h < w:\n",
    "        x = (w - h)//2\n",
    "        image = image[:, x:x+h, :]        \n",
    "    elif h > w:\n",
    "        x = (h - w)//2\n",
    "        image = image[x:x+w, :, :]           \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return image\n",
    "\n",
    "def scaleRadius (img, scale):\n",
    "    toto=int(img.shape[0]/2)\n",
    "    x=img[toto,:,:].sum (1)\n",
    "    r=(x>x.mean()/10).sum()/2\n",
    "    s=scale * 1.0/r\n",
    "    return cv2.resize (img,(0,0), fx=s , fy=s )\n",
    "\n",
    "def preprocess_image(file_name):\n",
    "    scale =320\n",
    "    # lecture de l'image\n",
    "    image = cv2.imread(os.path.join(IMG_DIR, file_name))\n",
    "\n",
    "    norm_img = np.zeros(image.shape)\n",
    "    # normalisation 0 ou 1\n",
    "    norm_img = cv2.normalize(image,  norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # enleve les bordures\n",
    "    image = crop(norm_img)\n",
    "\n",
    "    #redimensionnement de l'image à un rayon donné\n",
    "    image=scaleRadius (image, scale)\n",
    "    \n",
    "    #on soustrait la couleur moyenne pour la mapper sur 50% de gris de façon à mieux faire ressortir les constrastes\n",
    "    #image=cv2.addWeighted (image ,4,cv2.GaussianBlur (image,(0,0),scale/30),-4 ,128)\n",
    "    image=cv2.addWeighted (image ,4,cv2.GaussianBlur (image,(0,0),scale/30),-4 ,128)\n",
    "    \n",
    "    #on enleve 10% des bordures\n",
    "    b=np.zeros(image.shape)\n",
    "\n",
    "    cv2.circle(b,(int(image.shape[1]/2), int(image.shape[0]/2)),int(scale * 1),(1,1,1),-1,8,0)\n",
    "    image=image*b+128*(1-b)\n",
    "      \n",
    "    cv2.imwrite(str(scale)+\"_\"+file_name ,image)\n",
    "\n",
    "   \n",
    "    return image\n",
    "\n",
    "def preprocess_patient(patient_id):\n",
    "    left_eye_file = str(patient_id) + '_left.jpg'\n",
    "    right_eye_file = str(patient_id) + '_right.jpg'\n",
    "    # combine image vertical/horizontal\n",
    "    preprocess_image(left_eye_file)\n",
    "    #image = cv2.hconcat([preprocess_image(left_eye_file), preprocess_image(right_eye_file)]) \n",
    "\n",
    "    return image\n",
    "\n",
    "patient_id = main_df.iloc[8]['ID']\n",
    "image = preprocess_patient(patient_id)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "egyptian-doubt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_images/\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'filename'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-615a669c4588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m ])\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-213-615a669c4588>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, transform)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmy_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'*.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mfilename_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlabels_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'filename'"
     ]
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data_dir,transform):\n",
    "        'Initialization'\n",
    "\n",
    "        \"\"\"\n",
    "        to_drop = ['ID', 'Patient Age', 'Patient Sex', 'Left-Fundus', 'Right-Fundus',\n",
    "           'Left-Diagnostic Keywords', 'Right-Diagnostic Keywords', 'N', 'D', 'G',\n",
    "           'C', 'A', 'H', 'M', 'O', 'filepath', 'target']\n",
    "        data = data.drop(columns = to_drop)\n",
    "        \"\"\"\n",
    "\n",
    "        data = pd.read_csv('full_df.csv')\n",
    "\n",
    "        my_dir = data_dir+'preprocessed_images/'\n",
    "        print(my_dir)\n",
    "        my_list = glob.glob(os.path.join(my_dir,'*.jpg'))\n",
    "\n",
    "        filename_list = data['filename'].to_numpy()\n",
    "        labels_list = data['labels'].to_numpy()\n",
    "        print (labels_list)\n",
    "        self.labels = labels_list\n",
    "        self.list_IDs = filename_list\n",
    "        self.transform = transform\n",
    "        print(self.list_IDs[1:10])\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        'Generates one sample of data'\n",
    "        print(\"ICI\")\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "        # Load data and get label\n",
    "        X = torch.load('data/preprocessed_images/' + ID)\n",
    "        y = self.labels[ID]\n",
    "        return X, y\n",
    "        \"\"\"\n",
    "        ID = self.list_IDs[index]\n",
    "        img_path = 'preprocessed_images/' + ID\n",
    "        img = Image.open(img_path)\n",
    "        print (img_path)\n",
    "        img_transformed = self.transform(img)\n",
    "\n",
    "        labels_unique = np.unique(self.labels)\n",
    "        label = self.labels[index]\n",
    "        label = np.where(labels_unique == label)[0][0]\n",
    "        #print(self.label_list[idx], label)\n",
    "\n",
    "        return img_transformed, label\n",
    "\n",
    "\n",
    "\n",
    "my_transforms = transforms.Compose ([\n",
    "    transforms.RandomCrop((224,224)),\n",
    "    transforms.RandomRotation (degrees=45),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize (mean=[0.485,0.456,0.406],\n",
    "                          std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset=Dataset(data_dir='', transform=my_transforms)\n",
    "\n",
    "\n",
    "\n",
    "#img_num=0\n",
    "#for img, label in dataset:\n",
    "#    save_image (img, 'img'+str(img_num)+'.jpg')\n",
    "#    img_num +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "egyptian-heating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_images/\n",
      "['1_right.jpg' '2_right.jpg' '4_right.jpg' '5_right.jpg' '6_right.jpg'\n",
      " '7_right.jpg' '8_right.jpg' '9_right.jpg' '10_right.jpg']\n",
      "6392\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-a77ffc4066d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \"\"\"\n\u001b[1;32m    349\u001b[0m     \u001b[0;31m# Cannot verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sum of input lengths does not equal the length of the input dataset!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "my_transforms = transforms.Compose ([\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize (mean=[0.485,0.456,0.406],\n",
    "                          std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset=Dataset(data_dir='', transform=my_transforms)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-lease",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
